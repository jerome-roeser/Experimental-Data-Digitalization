{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental_data_digitalization.params import *\n",
    "from pypdf import PdfReader\n",
    "import pymupdf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/jacs.8b01774.pdf'),\n",
       " PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771-s1.pdf'),\n",
       " PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/ja8b01774_si_001.pdf'),\n",
       " PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/24gpt4v.pdf'),\n",
       " PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/d3dd00239j1.pdf'),\n",
       " PosixPath('/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pdfs = [file for file in LOCAL_DATA_PATH.iterdir() if file.suffix == '.pdf']\n",
    "pdfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract text (PyPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Image and data mining in reticular chemistry\\npowered by GPT-4V †\\nZhiling Zheng,abcZhiguo He,abOmar Khattab,dNakul Rampal,abcMatei A. Zaharia,e\\nChristian Borgs,ceJennifer T. Chayescefghand Omar M. Yaghi *abci\\nThe integration of arti ﬁcial intelligence into scienti ﬁc research opens new avenues with the advent of GPT-\\n4V, a large language model equipped with vision capabilities. In this study, we demonstrate that GPT-4V,accessible through the ChatGPT web user interface or an API, o ﬀers promising possibilities in navigating\\nand mining complex data for metal –organic frameworks (MOFs) especially from graphical sources ( e.g.\\nsorption isotherms, powder X-ray di ﬀraction patterns, thermogravimetric analysis graphs, etc.). Our\\napproach involved an automated process of converting 346 scholarly articles into 6240 images, whichrepresents a benchmark dataset in this task, followed by deploying GPT-4V to categorize and analyzethese images using natural language prompts, which can be written by chemists or materials scientists\\nwith minimal prior coding knowledge. This methodology enabled GPT-4V to accurately identify and\\ninterpret key plots integral to MOF characterization, such as nitrogen isotherms, PXRD patterns, and TGAcurves, among others, with accuracy and recall above 93%. The model's pro ﬁciency in extracting critical\\ninformation from these plots not only underscores its capability in data mining but also highlights itspotential to aid in the digitalization of experimental data and the creation of datasets for reticularchemistry. In addition, the trends and values of nitrogen isotherm data from the selected literatureallowed for a comparison between theoretical and experimental porosity values for over 200compounds, highlighting certain discrepancies and underscoring the importance of integrating\\ncomputational and experimental data. This work highlights the potential of AI in accelerating scienti ﬁc\\ndiscovery by bridging the gap between computational tools and experimental research.\\nIntroduction\\nThe integration of arti \\ue103cial intelligence (AI) with the chemical\\nsciences holds immense potential, and is accelerated by the rise\\nof large language models (LLMs).1–7These models have received\\nsubstantial attention for the fact that they can be intuitively“programmed ”or“taught ”using daily conversational language,\\nthereby assisting with diverse chemistry research tasks.\\n8–20It is\\nenvisioned that the evolution from text-only to more dynamic,multi-modal LLMs will result in even more powerful andconvenient AI assistants across various applications.\\n5,21 –23\\nThe recent introduction of GPT-4V, with ‘V’denoting its\\nvision capability, stands as a testament to this progress.21,24\\nTrained on a vast and varied collection of multi-modal data,\\nGPT-4V can process and respond to both textual and visualinputs.\\n24–27Its ability to interpret and analyze scienti \\ue103c litera-\\nture, especially in identifying valuable data within graphicalrepresentations, makes it more attractive than traditional text-only models in natural language processing (NLP).\\n10,28 –30These\\nnovel capabilities allow researchers from diverse backgrounds,\\nincluding those with no specialized coding or computer vision\\nexpertise, to harness the power of GPT-4V through customizedinstructions applicable to various areas.\\n19,31 –33aDepartment of Chemistry, University of California, Berkeley, California 94720, USA.\\nE-mail: yaghi@berkeley.edu\\nbKavli Energy Nanoscience Institute, University of California, Berkeley, California\\n94720, USA\\ncBakar Institute of Digital Materials for the Planet, University of California, Berkeley,\\nCalifornia 94720, USA\\ndDepartment of Computer Science, Stanford University, Stanford, California 94305,\\nUSA\\neDepartment of Electrical Engineering and Computer Sciences, University of California,\\nBerkeley, California 94720, USA\\nfDepartment of Mathematics, University of California, Berkeley, California 94720, USA\\ngDepartment of Statistics, University of California, Berkeley, California 94720, USA\\nhSchool of Information, University of California, Berkeley, California 94720, USA\\niKACST-UC Berkeley Center of Excellence for Nanomaterials for Clean Energy\\nApplications, King Abdulaziz City for Science and Technology, Riyadh 11442, Saudi\\nArabia\\n†Electronic supplementary information (ESI) available: Full prompts designed to\\nguide GPT-4V; additional examples showcasing GPT-4V's performance in reading\\nvarious \\ue103gure inputs and its corresponding responses; Python code used to\\nautomate the data mining and analysis processes; detailed information on theselected papers in this study, including the ground truth and the classi \\ue103cation\\noutput for each page in a spread-sheet format; extracted nitrogen isotherms in\\nthis study. See DOI: https://doi.org/10.1039/d3dd00239jCite this: Digital Discovery ,2 0 2 4 , 3,\\n491\\nReceived 9th December 2023\\nAccepted 1st February 2024\\nDOI: 10.1039/d3dd00239jrsc.li/digitaldiscovery\\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery ,2 0 2 4 , 3,4 9 1 –501 | 491Digital\\nDiscovery\\nPAPER\\nOpen Access Article. Published on 02 February 2024. Downloaded on 3/22/2024 3:01:49 PM. \\n This article is licensed under a \\nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\\nView Article Online\\nView Journal\\n | View Issue\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = PdfReader(pdfs[3])\n",
    "\n",
    "page = reader.pages[0]\n",
    "page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(pdfs[3]) # open a document\n",
    "with open(\"output_pypdf.txt\", \"wb\") as file: # create a text output\n",
    "    for page in reader.pages: # iterate the document pages\n",
    "        text = page.extract_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "        file.write(text) # write text of page\n",
    "        file.write(bytes((12,))) # write page delimiter (form feed 0x0C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract image (PyPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(pdfs[3])\n",
    "\n",
    "for page in reader.pages:\n",
    "    for count, image_file_object in enumerate(page.images):\n",
    "        with open(str(count) + image_file_object.name, \"wb\") as fp:\n",
    "            fp.write(image_file_object.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract text (PyMuPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image and data mining in reticular chemistry\n",
      "powered by GPT-4V†\n",
      "Zhiling Zheng,\n",
      "abc Zhiguo He,ab Omar Khattab,d Nakul Rampal,abc Matei A. Zaharia,e\n",
      "Christian Borgs,ce Jennifer T. Chayescefgh and Omar M. Yaghi\n",
      "*abci\n",
      "The integration of artiﬁcial intelligence into scientiﬁc research opens new avenues with the advent of GPT-\n",
      "4V, a large language model equipped with vision capabilities. In this study, we demonstrate that GPT-4V,\n",
      "accessible through the ChatGPT web user interface or an API, oﬀers promising possibilities in navigating\n",
      "and mining complex data for metal–organic frameworks (MOFs) especially from graphical sources (e.g.\n",
      "sorption isotherms, powder X-ray diﬀraction patterns, thermogravimetric analysis graphs, etc.). Our\n",
      "approach involved an automated process of converting 346 scholarly articles into 6240 images, which\n",
      "represents a benchmark dataset in this task, followed by deploying GPT-4V to categorize and analyze\n",
      "these images using natural language prompts, which can be written by chemists or materials scientists\n",
      "with minimal prior coding knowledge. This methodology enabled GPT-4V to accurately identify and\n",
      "interpret key plots integral to MOF characterization, such as nitrogen isotherms, PXRD patterns, and TGA\n",
      "curves, among others, with accuracy and recall above 93%. The model's proﬁciency in extracting critical\n",
      "information from these plots not only underscores its capability in data mining but also highlights its\n",
      "potential to aid in the digitalization of experimental data and the creation of datasets for reticular\n",
      "chemistry. In addition, the trends and values of nitrogen isotherm data from the selected literature\n",
      "allowed for a comparison between theoretical and experimental porosity values for over 200\n",
      "compounds,\n",
      "highlighting\n",
      "certain\n",
      "discrepancies\n",
      "and\n",
      "underscoring\n",
      "the\n",
      "importance\n",
      "of\n",
      "integrating\n",
      "computational and experimental data. This work highlights the potential of AI in accelerating scientiﬁc\n",
      "discovery by bridging the gap between computational tools and experimental research.\n",
      "Introduction\n",
      "The integration of articial intelligence (AI) with the chemical\n",
      "sciences holds immense potential, and is accelerated by the rise\n",
      "of large language models (LLMs).1–7 These models have received\n",
      "substantial attention for the fact that they can be intuitively\n",
      "“programmed” or “taught” using daily conversational language,\n",
      "thereby assisting with diverse chemistry research tasks.8–20 It is\n",
      "envisioned that the evolution from text-only to more dynamic,\n",
      "multi-modal LLMs will result in even more powerful and\n",
      "convenient AI assistants across various applications.5,21–23\n",
      "The recent introduction of GPT-4V, with ‘V’ denoting its\n",
      "vision capability, stands as a testament to this progress.21,24\n",
      "Trained on a vast and varied collection of multi-modal data,\n",
      "GPT-4V can process and respond to both textual and visual\n",
      "inputs.24–27 Its ability to interpret and analyze scientic litera-\n",
      "ture, especially in identifying valuable data within graphical\n",
      "representations, makes it more attractive than traditional text-\n",
      "only models in natural language processing (NLP).10,28–30 These\n",
      "novel capabilities allow researchers from diverse backgrounds,\n",
      "including those with no specialized coding or computer vision\n",
      "expertise, to harness the power of GPT-4V through customized\n",
      "instructions applicable to various areas.19,31–33\n",
      "aDepartment of Chemistry, University of California, Berkeley, California 94720, USA.\n",
      "E-mail: yaghi@berkeley.edu\n",
      "bKavli Energy Nanoscience Institute, University of California, Berkeley, California\n",
      "94720, USA\n",
      "cBakar Institute of Digital Materials for the Planet, University of California, Berkeley,\n",
      "California 94720, USA\n",
      "dDepartment of Computer Science, Stanford University, Stanford, California 94305,\n",
      "USA\n",
      "eDepartment of Electrical Engineering and Computer Sciences, University of California,\n",
      "Berkeley, California 94720, USA\n",
      "fDepartment of Mathematics, University of California, Berkeley, California 94720, USA\n",
      "gDepartment of Statistics, University of California, Berkeley, California 94720, USA\n",
      "hSchool of Information, University of California, Berkeley, California 94720, USA\n",
      "iKACST-UC Berkeley Center of Excellence for Nanomaterials for Clean Energy\n",
      "Applications, King Abdulaziz City for Science and Technology, Riyadh 11442, Saudi\n",
      "Arabia\n",
      "† Electronic supplementary information (ESI) available: Full prompts designed to\n",
      "guide GPT-4V; additional examples showcasing GPT-4V's performance in reading\n",
      "various gure inputs and its corresponding responses; Python code used to\n",
      "automate the data mining and analysis processes; detailed information on the\n",
      "selected papers in this study, including the ground truth and the classication\n",
      "output for each page in a spread-sheet format; extracted nitrogen isotherms in\n",
      "this study. See DOI: https://doi.org/10.1039/d3dd00239j\n",
      "Cite this: Digital Discovery, 2024, 3,\n",
      "491\n",
      "Received 9th December 2023\n",
      "Accepted 1st February 2024\n",
      "DOI: 10.1039/d3dd00239j\n",
      "rsc.li/digitaldiscovery\n",
      "© 2024 The Author(s). Published by the Royal Society of Chemistry\n",
      "Digital Discovery, 2024, 3, 491–501 | 491\n",
      "Digital\n",
      "Discovery\n",
      "PAPER\n",
      "Open Access Article. Published on 02 February 2024. Downloaded on 3/22/2024 3:01:49 PM. \n",
      " This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence.\n",
      "View Article Online\n",
      "View Journal  | View Issue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = pymupdf.open(pdfs[3]) # open a document\n",
    "with open(\"output_pymupdf.txt\", \"wb\") as file: # create a text output\n",
    "    for page in doc: # iterate the document pages\n",
    "        text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "        file.write(text) # write text of page\n",
    "        file.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "\n",
    "print(doc[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract images (PyMuPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images on page 0\n",
      "Found 2 images on page 1\n",
      "Found 1 images on page 2\n",
      "Found 1 images on page 3\n",
      "Found 2 images on page 4\n",
      "Found 2 images on page 5\n",
      "Found 2 images on page 6\n",
      "Found 1 images on page 7\n",
      "Found 1 images on page 8\n",
      "Found 1 images on page 9\n",
      "Found 1 images on page 10\n"
     ]
    }
   ],
   "source": [
    "for page_index in range(len(doc)): # iterate over pdf pages\n",
    "    page = doc[page_index] # get the page\n",
    "    image_list = page.get_images()\n",
    "\n",
    "    # print the number of images found on the page\n",
    "    if image_list:\n",
    "        print(f\"Found {len(image_list)} images on page {page_index}\")\n",
    "    else:\n",
    "        print(\"No images found on page\", page_index)\n",
    "\n",
    "    for image_index, img in enumerate(image_list, start=1): # enumerate the image list\n",
    "        xref = img[0] # get the XREF of the image\n",
    "        pix = pymupdf.Pixmap(doc, xref) # create a Pixmap\n",
    "\n",
    "        if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
    "            pix = pymupdf.Pixmap(pymupdf.csRGB, pix)\n",
    "\n",
    "        pix.save(\"page_%s-image_%s.png\" % (page_index, image_index)) # save the image as png\n",
    "        pix = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract Vector Graphics (PyMuPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page-index 0: 52 vector graphics\n",
      "page-index 1: 16 vector graphics\n",
      "page-index 2: 16 vector graphics\n",
      "page-index 3: 19 vector graphics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['items', 'type', 'even_odd', 'fill_opacity', 'fill', 'rect', 'seqno', 'layer', 'closePath', 'color', 'width', 'lineCap', 'lineJoin', 'dashes', 'stroke_opacity']),\n",
       " [(1.0, 1.0, 1.0),\n",
       "  (1.0, 1.0, 1.0),\n",
       "  (0.5071183443069458, 0.49796292185783386, 0.6900892853736877),\n",
       "  (0.5071183443069458, 0.49796292185783386, 0.6900892853736877),\n",
       "  (0.15284962952136993, 0.19644464552402496, 0.49930572509765625),\n",
       "  (1.0, 1.0, 1.0),\n",
       "  (0.15284962952136993, 0.19644464552402496, 0.49930572509765625),\n",
       "  (1.0, 1.0, 1.0),\n",
       "  None,\n",
       "  None,\n",
       "  None])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pymupdf.open(pdfs[0])\n",
    "for number, page in enumerate(doc):\n",
    "    paths = page.get_drawings()\n",
    "    print(f\"page-index {number}: {len(paths)} vector graphics\")\n",
    "\n",
    "page = doc[0]\n",
    "paths = page.get_drawings()\n",
    "paths[0].keys(), [path.get('fill', 'is None') for path in paths if path.get('fill')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 40\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munhandled drawing\u001b[39m\u001b[38;5;124m\"\u001b[39m, item)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# all items are drawn, now apply the common properties\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# to finish the path\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     shape\u001b[38;5;241m.\u001b[39mfinish(\n\u001b[1;32m     34\u001b[0m         fill\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# fill color\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         color\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# line color\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         dashes\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashes\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# line dashing\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         even_odd\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven_odd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# control color of overlaps\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         closePath\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosePath\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# whether to connect last and first point\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         lineJoin\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineJoin\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# how line joins should look like\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m         lineCap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlineCap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# how line ends should look like\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         width\u001b[38;5;241m=\u001b[39mpath[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# line width\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         stroke_opacity\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstroke_opacity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# same value for both\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         fill_opacity\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill_opacity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# opacity parameters\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# all paths processed - commit the shape to its page\u001b[39;00m\n\u001b[1;32m     46\u001b[0m shape\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "page = doc[0]\n",
    "paths = page.get_drawings()  # extract existing drawings\n",
    "# this is a list of \"paths\", which can directly be drawn again using Shape\n",
    "# -------------------------------------------------------------------------\n",
    "#\n",
    "# define some output page with the same dimensions\n",
    "outpdf = pymupdf.open()\n",
    "outpage = outpdf.new_page(width=page.rect.width, height=page.rect.height)\n",
    "shape = outpage.new_shape()  # make a drawing canvas for the output page\n",
    "# --------------------------------------\n",
    "# loop through the paths and draw them\n",
    "# --------------------------------------\n",
    "for path in paths:\n",
    "    # ------------------------------------\n",
    "    # draw each entry of the 'items' list\n",
    "    # ------------------------------------\n",
    "\n",
    "    for item in path[\"items\"]:  # these are the draw commands\n",
    "        if item[0] == \"l\":  # line\n",
    "            shape.draw_line(item[1], item[2])\n",
    "        elif item[0] == \"re\":  # rectangle\n",
    "            shape.draw_rect(item[1])\n",
    "        elif item[0] == \"qu\":  # quad\n",
    "            shape.draw_quad(item[1])\n",
    "        elif item[0] == \"c\":  # curve\n",
    "            shape.draw_bezier(item[1], item[2], item[3], item[4])\n",
    "        else:\n",
    "            raise ValueError(\"unhandled drawing\", item)\n",
    "    # ------------------------------------------------------\n",
    "    # all items are drawn, now apply the common properties\n",
    "    # to finish the path\n",
    "    # ------------------------------------------------------\n",
    "    shape.finish(\n",
    "        fill=path[\"fill\"],  # fill color\n",
    "        color=path[\"color\"],  # line color\n",
    "        dashes=path[\"dashes\"],  # line dashing\n",
    "        even_odd=path.get(\"even_odd\", True),  # control color of overlaps\n",
    "        closePath=path[\"closePath\"],  # whether to connect last and first point\n",
    "        lineJoin=path[\"lineJoin\"],  # how line joins should look like\n",
    "        lineCap=max(path[\"lineCap\"]),  # how line ends should look like\n",
    "        width=path[\"width\"],  # line width\n",
    "        stroke_opacity=path.get(\"stroke_opacity\", 1),  # same value for both\n",
    "        fill_opacity=path.get(\"fill_opacity\", 1),  # opacity parameters\n",
    "        )\n",
    "# all paths processed - commit the shape to its page\n",
    "shape.commit()\n",
    "outpdf.save(\"drawings-page-0.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 found on page 0 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 1 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 2 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 3 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 4 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 5 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n",
      "0 found on page 6 of /home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/Experimental-Data-Digitalization/data/nchem.2771.pdf\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "doc = pymupdf.open(pdfs[5])\n",
    "for idx, page in enumerate(doc):\n",
    "    # page = doc[0] # get the 1st page of the document\n",
    "    tabs = page.find_tables() # locate and extract any tables on page\n",
    "    print(f\"{len(tabs.tables)} found on {page}\") # display number of found tables\n",
    "\n",
    "    if tabs.tables:  # at least one table found?\n",
    "     pprint(tabs[0].extract())  # print content of first table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimental-data-digitalization-KoFsBjW7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
